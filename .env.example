# ── Modelo LLM ─────────────────────────────────────────────
MODEL_PATH=./models/Qwen_Qwen3-1.7B-Q4_K_M.gguf
N_THREADS=3
N_CTX=2048
N_BATCH=256
MAX_TOKENS=256
TEMPERATURE=0.7

# ── ASR (Faster-Whisper) ──────────────────────────────────
WHISPER_MODEL=base
WHISPER_LANGUAGE=es

# ── TTS (Piper) ───────────────────────────────────────────
PIPER_VOICE=./voices/es_ES-davefx-medium.onnx

# ── Servidor ───────────────────────────────────────────────
HOST=0.0.0.0
PORT=8080

# ── Historial de conversacion ──────────────────────────────
MAX_HISTORY_TURNS=10
HISTORY_FILE=./conversation_history.json

# ── Pipelines activos ─────────────────────────────────────
# Lista separada por comas de los pipelines a cargar.
# Pipelines disponibles: robot, assistant
# Dejar vacio para un servidor solo con /health.
# Ejemplo: PIPELINES=robot,assistant
PIPELINES=

# ── System prompts (opcionales) ───────────────────────────
# Cada pipeline trae un prompt por defecto. Puedes sobreescribirlo
# definiendo <NOMBRE>_SYSTEM_PROMPT aqui. Si no se define, se usa
# el prompt por defecto del pipeline.
#
# ROBOT_SYSTEM_PROMPT=<tu prompt personalizado>
# ASSISTANT_SYSTEM_PROMPT=<tu prompt personalizado>
